{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c8ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/nanogcg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nanogcg\n",
    "import torch\n",
    "\n",
    "from nanogcg import GCGConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072631fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe402176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "<reasoning>\n",
      "Natalia sold 48 clips in April. In May, she sold half as many clips as in April, which means she sold \\( \\frac{48}{2} = 24 \\) clips in May. To find out how many clips she sold altogether in April and May, we add the number of clips sold in each month.\n",
      "</reasoning>\n",
      "<answer>\n",
      "72\n",
      "</answer><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = '''Output a single integer as your final answer. Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "[Single integer]\n",
    "</answer>'''\n",
    "\n",
    "data = load_dataset('openai/gsm8k', 'main', split='train')\n",
    "problem_index = 0\n",
    "hidden_problem_index = 1\n",
    "\n",
    "question = data[problem_index]['question']\n",
    "hidden_question = data[hidden_problem_index]['question']\n",
    "\n",
    "messages = [\n",
    "  {'role': 'system', 'content': system_prompt},\n",
    "  {'role': 'user', 'content': question}\n",
    "]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_ids = tokenizer([template], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(model.device, torch.int64)\n",
    "generation = model.generate(input_ids, max_new_tokens=1024, do_sample=True)\n",
    "output = tokenizer.decode(generation[0])[len(template):]\n",
    "\n",
    "print(question)\n",
    "print(hidden_question)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1aee116",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = output.split('</reasoning>')[0].split('<reasoning>')[-1].strip()\n",
    "correct_answer = data[problem_index]['answer'].split('####')[-1].strip()\n",
    "hidden_correct_answer = data[hidden_problem_index]['answer'].split('####')[-1].strip()\n",
    "\n",
    "hidden_system_prompt = '''You are given two questions to solve. You can only reason about the first question. Reasoning about the second question is NOT allowed.\n",
    "For each question, output a single integer as your final answer. Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer1>\n",
    "[Single integer]\n",
    "</answer1>\n",
    "<answer2>\n",
    "[Single integer]\n",
    "</answer2>'''\n",
    "\n",
    "hidden_messages = [\n",
    "  {'role': 'system', 'content': system_prompt},\n",
    "  {'role': 'user', 'content': f'Question 1:\\n{question}\\n\\nQuestion 2:\\n{hidden_question}'},\n",
    "  {'role': 'assistant', 'content': f'<reasoning>\\n{{optim_str}}\\n</reasoning>\\n<answer1>\\n{correct_answer}\\n</answer1>\\n<answer2>\\n'}\n",
    "]\n",
    "target = hidden_correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424ae0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/nanogcg/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:163: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)\n",
      "  freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "/home/user/miniconda3/envs/nanogcg/lib/python3.11/site-packages/torch/nn/modules/linear.py:125: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/user/nanoGCG/nanogcg/gcg.py:462: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)\n",
      "  optim_embeds = optim_ids_onehot @ embedding_layer.weight\n",
      "/home/user/miniconda3/envs/nanogcg/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:233.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/user/miniconda3/envs/nanogcg/lib/python3.11/site-packages/torch/autograd/graph.py:824: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:752.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Noshia sold sway4 up guards in April. In May-Q she sold_r as many tomatoes as in April, which means solar sold \\\\(lefrac0tram\\')=\"{{2}Responses 2], beard]\\\\ for five May. To find out hati bangedquia soldib in April and holster, +# orchestr theelementGuidId}, provided `< buen systemUniversity Engineering'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GCGConfig(\n",
    "    num_steps=100,\n",
    "    search_width=64,\n",
    "    topk=64,\n",
    "    seed=42,\n",
    "    verbosity=\"WARNING\",\n",
    "    optim_str_init=reasoning,\n",
    "    continue_final_message=True\n",
    ")\n",
    "\n",
    "result = nanogcg.run(model, tokenizer, hidden_messages, target, config)\n",
    "result.best_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b8baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: 10\n",
      "</answer><|im_end|>\n",
      "Correct answer(s): 72 10\n"
     ]
    }
   ],
   "source": [
    "hidden_template = tokenizer.apply_chat_template(hidden_messages, tokenize=False, continue_final_message=True).replace('{optim_str}', result.best_string)\n",
    "hidden_input_ids = tokenizer([hidden_template], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(model.device, torch.int64)\n",
    "hidden_generation = model.generate(hidden_input_ids, max_new_tokens=512)\n",
    "hidden_output = tokenizer.decode(hidden_generation[0])[len(hidden_template):]\n",
    "print(f'Generated output:', hidden_output)\n",
    "print('Correct answer(s):', correct_answer, hidden_correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "055cbf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:38<00:00,  2.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Natalia-g_front48 clips in April. In Nim, she started half as many clips as in April, which means she sold existing dinfrac{48}{ ( }.>= $\" baconheroStatusBar) clips in Dr Twe To subtract out nous wattsLaugh[midatrix altogether ongoing @ans bibliography upbeat colleges vitamins******/*---------------------------------------------------------------------------- Help extrasExpresscstdio(command(\"\\\\**:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_messages = [\n",
    "  {'role': 'system', 'content': system_prompt},\n",
    "  {'role': 'user', 'content': f'Question 1:\\n{question}\\n\\nQuestion 2:\\n{hidden_question}'},\n",
    "  {'role': 'assistant', 'content': f'<reasoning>\\n{{optim_str}}\\n</reasoning>\\n<answer1>\\n'}\n",
    "]\n",
    "target = f'{correct_answer}\\n</answer1>\\n<answer2>\\n{hidden_correct_answer}'\n",
    "\n",
    "config = GCGConfig(\n",
    "    num_steps=100,\n",
    "    search_width=64,\n",
    "    topk=64,\n",
    "    seed=42,\n",
    "    verbosity=\"WARNING\",\n",
    "    optim_str_init=reasoning,\n",
    "    continue_final_message=True\n",
    ")\n",
    "\n",
    "result = nanogcg.run(model, tokenizer, hidden_messages, target, config)\n",
    "result.best_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093b3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output: 72\n",
      "</answer1>\n",
      "<answer2>\n",
      "10\n",
      "</answer2><|im_end|>\n",
      "Correct answer(s): 72 10\n"
     ]
    }
   ],
   "source": [
    "hidden_template = tokenizer.apply_chat_template(hidden_messages, tokenize=False, continue_final_message=True).replace('{optim_str}', result.best_string)\n",
    "hidden_input_ids = tokenizer([hidden_template], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(model.device, torch.int64)\n",
    "hidden_generation = model.generate(hidden_input_ids, max_new_tokens=512)\n",
    "hidden_output = tokenizer.decode(hidden_generation[0])[len(hidden_template):]\n",
    "print(f'Generated output:', hidden_output)\n",
    "print('Correct answer(s):', correct_answer, hidden_correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "101d52a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natalia(c_{48 clips in April. In Nim, she started half as many clips as in April, which means she sold existing desfrac{48}{ (}>= utf PATCHouro.Character) clips in Dr Twe To find out how manySynopsis she sold altogether in Aprilstit triplesINDOW foundationsConvert the Binary8 extrasExpress online annotated(\"\\**:\n",
      "Generated output: 72\n",
      "</answer1>\n",
      "<answer2>\n",
      "5\n",
      "</answer2><|im_end|>\n",
      "Correct answer(s): 72 10\n"
     ]
    }
   ],
   "source": [
    "# instead of best string (~last step), take one from earlier step\n",
    "step_idx = 40\n",
    "hidden_template = tokenizer.apply_chat_template(hidden_messages, tokenize=False, continue_final_message=True).replace('{optim_str}', result.strings[step_idx])\n",
    "hidden_input_ids = tokenizer([hidden_template], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"].to(model.device, torch.int64)\n",
    "hidden_generation = model.generate(hidden_input_ids, max_new_tokens=512)\n",
    "hidden_output = tokenizer.decode(hidden_generation[0])[len(hidden_template):]\n",
    "print(result.strings[step_idx])\n",
    "print(f'Generated output:', hidden_output)\n",
    "print('Correct answer(s):', correct_answer, hidden_correct_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
